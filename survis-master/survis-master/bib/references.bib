@INPROCEEDINGS{10101569,
  author={Islam, Niful and Fatema-Tuj-Jahra, Most. and Hasan, Md. Tarek and Farid, Dewan Md.},
  booktitle={2023 International Conference on Electrical, Computer and Communication Engineering (ECCE)}, 
  title={{KNNTree}: A New Method to Ameliorate K-Nearest Neighbour Classification using Decision Tree}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={Classification in supervised learning is one of the major issues in machine learning and data science. K-Nearest Neighbour (KNN) and Decision Tree (DT) are one of the most widely used classification techniques that are commonly applying for single model and ensemble processes. KNN is known as lazy learner as it doesn't build any decision line from the training data. DT, on the other hand, is a top-down recursive divide-and-conquer technique that used for both classification and regression problems. DT has several advantages e.g, is requires little prior knowledge and non-linear relationship of features don't affect the tree performance. In this paper, we have proposed a new learning algorithm named KNNTree which is a hybrid model of KNN and DT algorithms. The proposed model is basically a decision tree, but leaf nodes are replaced by the KNN classifier. We have tested the proposed method with KNN and DT algorithms on 10 benchmark datasets taken from UC Irvine Machine Learning Repository and found the proposed method outperforms both KNN and DT classifiers.},
  keywords={},
  doi={10.1109/ECCE57851.2023.10101569},
  ISSN={},
  month={Feb},}

@INPROCEEDINGS{9864503,
  author={Farid, Dewan Md. and Sworna, Nabila Sabrin and Amin, Ruhul and Sadia, Nazifa and Rahman, Moshiur and Liton, Nazmul Khan and Hossain Mukta, Md. Saddam and Shatabda, Swakkhar},
  booktitle={2022 IEEE Region 10 Symposium (TENSYMP)}, 
  title={{Boosting K-Nearest Neighbour (KNN) Classification} using Clustering and AdaBoost Methods}, 
  year={2022},
  volume={},
  number={},
  pages={1-6},
  abstract={K-Nearest Neighbour (KNN) is a lazy learning algorithm which has been successfully practiced in several real-life machine learning applications. However, it has been titled as lazy learner because it is unable to learn a decision line from historical training data and uses the data itself for classification. KNN discovers the closest neighbors around test instances; At the same time considers the majority voting of the class label to classify the test instances. In the case of noisy datasets, the value of K should be higher. It is still struggling with the time complexity and better accuracy rate in some cases for noisy datasets. In this article, we proposed a technique to boost up the performance of KNN classification employing clustering and boosting techniques. The proposed method automatically determine the value of K based on the current situation of the neighbours instead of setting previous and random values. It also achieves significant accuracy and reduces the time complexity in compare with traditional KNN approach. The proposed method clusters the datasets and finds the majority voting of nearest neighbours from each cluster to classify the test data. We experimented the performance of suggested model on several real benchmark datasets from UCI and KEEL machine learning repositories and found that proposed model gives significant results which are relatively satisfactory in terms of accuracy, precision, f'1-score and Matthew's Correlation Coefficient (MCC).},
  keywords={},
  doi={10.1109/TENSYMP54529.2022.9864503},
  ISSN={2642-6102},
  month={July},}

@INPROCEEDINGS{7100689,
  author={Taneja, Shweta and Gupta, Charu and Aggarwal, Sakshi and Jindal, Veni},
  booktitle={2015 International Conference on Cognitive Computing and Information Processing(CCIP)}, 
  title={{MFZ-KNN } , A modified fuzzy based K nearest neighbor algorithm}, 
  year={2015},
  volume={},
  number={},
  pages={1-5},
  abstract={KNN is amongst the simplest top ten classification algorithm of data mining. Being effective and efficient it has some drawbacks which cannot be overlooked. Moreover, real world data is fuzzy in nature. To overcome this drawback fuzzy KNN was introduced which was based on fuzzy membership. But, it had large time complexity as the membership is calculated at the classification period. To improve this, we have proposed a modified fuzzy based KNN algorithm MFZ-KNN whereby fuzzy clusters are obtained at preprocessing step and the membership of the training data set is computed in reference with the centroid of the clusters. This reduces the complexity of time remarkably. We have implemented the algorithm in MatLAB and Netbeans IDE using standard UCI data set-Wine. The results prove that it is better than both conventional KNN and fuzzy KNN in terms of accuracy and time.},
  keywords={},
  doi={10.1109/CCIP.2015.7100689},
  ISSN={},
  month={March},}

@article{LIAO2002439,
title = {Use of {K-Nearest Neighbor} classifier for intrusion detection11An earlier version of this paper is to appear in the Proceedings of the 11th USENIX Security Symposium, San Francisco, CA, August 2002},
journal = {Computers & Security},
volume = {21},
number = {5},
pages = {439-448},
year = {2002},
issn = {0167-4048},
doi = {https://doi.org/10.1016/S0167-4048(02)00514-X},
url = {https://www.sciencedirect.com/science/article/pii/S016740480200514X},
author = {Yihua Liao and V.Rao Vemuri},
keywords = {k-Nearest Neighbor classifier, intrusion detection, system calls, text categorization, program profile.},
abstract = {A new approach, based on the k-Nearest Neighbor (kNN) classifier, is used to classify program behavior as normal or intrusive. Program behavior, in turn, is represented by frequencies of system calls. Each system call is treated as a word and the collection of system calls over each program execution as a document. These documents are then classified using kNN classifier, a popular method in text categorization. This method seems to offer some computational advantages over those that seek to characterize program behavior with short sequences of system calls and generate individual program profiles. Preliminary experiments with 1998 DARPA BSM audit data show that the kNN classifier can effectively detect intrusive attacks and achieve a low false positive rate.}
}

@INPROCEEDINGS{8079924,
  author={Moldagulova, Aiman and Sulaiman, Rosnafisah Bte.},
  booktitle={2017 8th International Conference on Information Technology (ICIT)}, 
  title={Using {KNN} algorithm for classification of textual documents}, 
  year={2017},
  volume={},
  number={},
  pages={665-671},
  abstract={Nowadays the exponential growth of generation of textual documents and the emergent need to structure them increase the attention to the automated classification of documents into predefined categories. There is wide range of supervised learning algorithms that deal with text classification. This paper deals with an approach for building a machine learning system in R that uses K-Nearest Neighbors (KNN) method for the classification of textual documents. The experimental part of the research was done on collected textual documents from two sources: http://egov.kz and http://www.government.kz. The experiment was devoted to challenging thing of the KNN algorithm that to find the proper value of k which represents the number of neighbors.},
  keywords={},
  doi={10.1109/ICITECH.2017.8079924},
  ISSN={},
  month={May},}


@article{Shah2020ACA,
  title={A Comparative Analysis of Logistic Regression, Random Forest and KNN Models for the Text Classification},
  abstract={In the current generation, a huge amount of textual documents are generated and there is an urgent need to organize them in a proper structure so that classification can be performed and categories can be properly defined. The key technology for gaining the insights into a text information and organizing that information is known as text classification. The classes are then classified by determining the text types of the content. Based on different machine learning algorithms used in the current paper, the system of text classification is divided into four sections namely text pre-treatment, text representation, implementation of the classifier and classification. In this paper, a BBC news text classification system is designed. In the classifier implementation section, the authors separately chose and compared logistic regression, random forest and K-nearest neighbour as our classification algorithms. Then, these classifiers were tested, analysed and compared with each other and finally got a conclusion. The experimental conclusion shows that BBC news text classification model gets satisfying results on the basis of algorithms tested on the data set. The authors decided to show the comparison based on five parameters namely precision, accuracy, F1-score, support and confusion matrix. The classifier which gets the highest among all these parameters is termed as the best machine learning algorithm for the BBC news data set.},
  author={Kanish Shah and Henil Patel and Devanshi Sanghvi and Manan Shah},
  journal={Augmented Human Research},
  year={2020},
  keywords= {Text classification,Machine learning algorithm,Logistic regression,Random forest,K-nearest neighbour,Natural language processing,Feature extraction},
  volume={5},
  pages={1-16},
  doi={10.1007/s41133-020-00032-0}
}


@INPROCEEDINGS{9432178,
  author={C, Chethana},
  booktitle={2021 5th International Conference on Intelligent Computing and Control Systems (ICICCS)}, 
  title={Prediction of Heart Disease using Different {KNN Classifier}}, 
  year={2021},
  volume={},
  number={},
  pages={1186-1194},
  abstract={Recently, Machine learning classification algorithms are playing a vital role in analysing various data available in cloud storage and websites. In this paper, the heart disease dataset is considered and the results are predicted by using various version of the KNN classifier in MATLAB. The comparisons of the performance of all these algorithms have been evaluated for accuracy, misclassification rate. The distance metric and distance weight used by various KNN algorithms is also shown. The experiment has done with the PCA option enabled. The optimized KNN was best with an accuracy of 69 with a prediction speed of ~5600 obs/sec.},
  doi={10.1109/ICICCS51141.2021.9432178},
  ISSN={},
  month={May},
  }


@INPROCEEDINGS{6755106,
  author={Babu, U. Ravi and Venkateswarlu, Y. and Chintha, Aneel Kumar},
  booktitle={2014 World Congress on Computing and Communication Technologies}, 
  title={Handwritten Digit Recognition Using {K-Nearest Neighbour Classifier}}, 
  year={2014},
  volume={},
  number={},
  pages={60-65},
  abstract={This paper presents a new approach to off-line handwritten digit recognition based on structural features which is not required thinning operation and size normalization technique. In this paper uses four different types of structural features namely, number of holes, water reservoirs in four directions, maximum profile distances in four directions, and fill-hole density for the recognition of digits. The digit recognition system mainly depends on which kinds of features are used. The main objective of this paper is to provide efficient and reliable techniques for recognition of handwritten digits. A Euclidean minimum distance criterion is used to find minimum distances and k-nearest neighbor classifier is used to classify the digits. A MNIST database is used for both training and testing the system. 5000 images are used to test the proposed method a total 5000 numeral images are tested and got 96.94 recognition rate.},
  keywords={},
  doi={10.1109/WCCCT.2014.7},
  ISSN={},
  month={Feb},}


@INPROCEEDINGS{9633738,
  author={Jiang, Huihai},
  booktitle={2021 IEEE 3rd International Conference on Civil Aviation Safety and Information Technology (ICCASIT)}, 
  title={Cryptocurrency price forecasting based on shortterm trend KNN model}, 
  year={2021},
  volume={},
  number={},
  pages={1165-1169},
  abstract={Since the appearance of bitcoin, the cryptocurrency market has gradually prospered. The price of cryptocurrency often changes rapidly and fluctuates greatly, which makes many investors rush into the market, ignoring the risk behind it. The price prediction of cryptocurrency can not only provide a reference for investors' investment but also reveal its financial laws to a certain extent and improve the risk early warning mechanism as well as improve its stability and security. In this paper, the KNN algorithm is used to forecast the cryptocurrency price, and the KNN model is improved based on the short-term price trend before the trading day. The experimental results show that the improved KNN model has a more accurate prediction result than the logistic regression model and traditional KNN model.},
  keywords={},
  doi={10.1109/ICCASIT53235.2021.9633738},
  ISSN={},
  month={Oct},}

@article{10.1162/tacl_a_00356,
    author = {Fan, Angela and Gardent, Claire and Braud, Chloé and Bordes, Antoine},
    title = {Augmenting Transformers with {KNN-Based Composite Memory for Dialog}},
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {9},
    pages = {82-99},
    year = {2021},
    month = {03},
    abstract = "{Various machine learning tasks can benefit from access to external information of different modalities, such as text and images. Recent work has focused on learning architectures with large memories capable of storing this knowledge. We propose augmenting generative Transformer neural networks with KNN-based Information Fetching (KIF) modules. Each KIF module learns a read operation to access fixed external knowledge. We apply these modules to generative dialog modeling, a challenging task where information must be flexibly retrieved and incorporated to maintain the topic and flow of conversation. We demonstrate the effectiveness of our approach by identifying relevant knowledge required for knowledgeable but engaging dialog from Wikipedia, images, and human-written dialog utterances, and show that leveraging this retrieved information improves model performance, measured by automatic and human evaluation.}",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00356},
    url = {https://doi.org/10.1162/tacl\_a\_00356},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00356/1924032/tacl\_a\_00356.pdf},
}



